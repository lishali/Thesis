\chapter{The Graph Neural Network Model}


\section{Graph Neural Network}

The Graph Neural Network (GNN) is a flexible neural network architecture that is based on two local operators on the a graph $G = (V, E)$. Given some input signal $F \in \mathbb{R}^{n \times p}$ on the vertices of an n-vertex graph, we consider two operators that acts locally on this signal as well as a non-linearity.\\


\noindent D: Diagonal matrix of degrees of $V$.  Define $(DF)_i = deg(i) \times F_i$.  \\
W: Adjacency matrix of $G$.  Define $(WF)_i:= \sum_{j\sim i }F_j.$\\
\textbf{$\theta$}: Pointwise Non-Linearity For each layer of the graph neural network, and for each node, we have $\eta_{\theta}: \mathbb{R}^p \times \mathbb{R}^p \rightarrow \mathbb{R}^q$ parametrized by $\theta$ trainable. Thus each node $i \in G$ undergoes $g_i = \eta_{\theta}(((DF)_i, (WF)_i)$\\


\begin{figure}
\begin{center}
  \includegraphics[width=\textwidth]{GNN.png}
  \caption{An example of an architecture constructed from operators D, W and $\theta$}
  \label{fig:GNN}
 \end{center}
\end{figure}

Immediately from this definition, we get that after one layer of applying $D$ and $W$ allows us to recover the graph Laplacian operator.  To be precise, let $F$ be a $p$-dim signal on $G$.  Then if we define $$ \eta(DF, WF) := DF - WF$$ we recover the unnormalized graph Laplacian.  If we allow ourselves preprocessing to also define $D^{-1/2}$ and $D^{-1}$, we will be able to recover the symmetric and random walk Laplacians.  Furthermore, by stacking together several of the Laplacian modules, and allowing ourselves to renormalize the signal after each  module, we are able to recreate the power method.  Therefore the expressive power of this architecture includes approximations to eigendecompositions.\\

\subsection{Related Work}
The GNN was first proposed in \cite{GNN} as a way to approximate functions on graphs. Bruna et al. also generalized convolutions for signals on graphs\cite{Bruna}.  The idea there is that the convolution neural network architecture so successful for image data, can be construed as decomposing the image signals into the very rapidly decaying Fourier basis.  Rapidly decaying because images lie on such regular graphs (in particular grids). The right generalization is then to use the graph Laplacian's eigenbasis to create a general graph convolution.  The authors successfully applied this neural network for signals on meshes.  Kipf and Welling showed more recently in \cite{kipf2016semi} the GNN with a symmetric Laplacian module, with only two layers can be quite effective as a embedding mechanism for graph signals.  And applied their network to semi-supervised learning problems where some graph nodes were labelled but others were not.\\

The present work is the first time a neural network has been applied to community detection.  In particular this means the graphs we are dealing with are far larger and far sparser then all previous works.  The GNN architecture family is quite general, so it required taking the most expressive combination of modules to include in it's expressive closure spectral algorithms of interest mentioned in the last chapter.  We are able to show that our version of the GNN can compete with spectral algorithms in doing spectral clustering on the SBM in even the hardest of regimes (detectability).  This will not work with previous GNN architectures mentioned above.  In addition, we are not doing an eigendecomposition, which is required of spectral algorithms.  Thus making the network more efficient computationally.  %Lastly we apply the GNN to extremely large graphs, with millions of nodes.  These real datasets have ground truth communities which we compare the performance of.  



