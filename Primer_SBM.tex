\chapter{A Primer on Clustering in the SBM}

Two challenges before us in studying clustering procedures rigorously are: 
\begin{center}
What is the ground truth?\\

What are the algorithmic guarantees?
\end{center}
Approached from a theoretical point of view, progress on these questions comes from studying special cases, oftentimes derived from generative probability models that capture much of the empirical behaviour.  One extremely well studied model is the Stochastic Blockmodel (SBM).  One quick Google scholar search reveals about 8000 papers written on this topic, around 3000 of which since 2014.  It's a particularly simple and natural extension of the Erd\H{o}s Renyi random graph model, which exhibits community structure.  The randomness involved in the construction of the graph allows one to prove properties of the graph that hold asymptotically.  Despite the simplicity of its definition (requiring few parameters), the SBM has proved fertile grounds for testing algorithmic performance.  Theoretical questions to do with community detectability thresholds for the model given certain parameters have only been addressed in the easiest of regimes (2 communities)  in the last couple of years, using techniques ranging from branching processes in probability theory to phase transitions in Ising-like models in statistical physics.  

\begin{definition}The \textit{Stochastic Block Model (SBM)} is a random graph model that can be defined by the following three parameters.
\begin{align*}
    n & : \text{ The number of vertices in the graph. }\\
    c & =(c_1, ...c_k): \text{ A partition of the vertex set,} \{1,2, ...,n\} \text{, into } k \text{ disjoint subsets. }\\
    W & \in \mathbb{R}^{k \times k}_{\geq 0}: \text{A symmetric matrix of probabilities of connections between the } k \text{ communitites.}
\end{align*}

Given the above parameters, one can sample a SBM$(n, c, W)$ graph, call it $G = (V, E)$ (where $V$ is the vertex set and $E$ is the edge set, and $n= |V|$) by connecting two vertices $u, v \in V$ with probability $W_{ij}$, where $W_{ij}$ is the $ij^{th}$ entry of $W$, $v \in c_i$ and $u \in c_j$.  Whether one edge is in the SBM or not is independent of other edges and is solely determined by $W$ and $c$.  
\end{definition}

In the easiest case with two communities of the same size, we can define the SBM with 3 scalar paramters $n$, $p, q$, where $n$ is the number of vertices, $p$ is the probability of connecting two vertices if they are from the same community and $q$ is the probability of connecting between two nodes of different communities. 


Since the SBM is a random graph, a given set of parameters will give rise to a distribution of graphs.  For instance, the following three graphs come from the same parameters: $n = 8$, $c = (\{0,1,2,3\}, \{4,5,6,7\})$, and $W =([1,0.15],[0.15,1])$. Otherwise put, in the simpler $3$-scalar parametrization, we have that Figure \ref{fig:awesome_image1}, \ref{fig:awesome_image2} and \ref{fig:awesome_image3} are all instantiations of SBM with parameters $p=1.0$, $q=0.15$ and $n=8$.  
\begin{figure}[h]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{labels_and_colors_1.png}
  \caption{Instantiation 1 of SBM with $p=1.0$, $q=0.15$.}\label{fig:awesome_image1}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{labels_and_colors_2.png}
  \caption{Instantiation 2 of SBM with $p=1.0$, $q=0.15$.}\label{fig:awesome_image2}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{labels_and_colors_3.png}
  \caption{Instantiation 3 of SBM with $p=1.0$, $q=0.15$.}\label{fig:awesome_image3}
\endminipage
\end{figure}


So why is the clustering on the SBM hard?  Clusterings for the instantiations above can be convincingly eye-balled.  Let's consider another example.  

\begin{figure}[h]
\begin{center}
  \includegraphics[scale=0.5]{SBM}
  \caption{Coloured and ordered adjacency matrix of SBM. Figure taken from \cite{SBM_adjacency_talk}}
  \label{fig:SBM_matrix_colour}
 \end{center}
\end{figure}

The figure \ref{fig:SBM_matrix_colour} is a coloured adjacency matrix representation of an SBM graph. Here we represent the adjacency matrix by colouring a square in \ref{fig:SBM_matrix_colour} if there is an edge, and leaving it white if there is no edge.  The red and yellow colouring differentiates the different community membership relationships (red for edges that are between two nodes of the same community, and yellow for edges between nodes that differ in their community membership). The number of nodes $n$ is much bigger in \ref{fig:SBM_matrix_colour} than in \ref{fig:awesome_image1}. Clearly $p$ and $q$ also don't seem too far apart, since the density of red and yellow nodes, though the difference is still perceptible, is not a great difference.  In a real clustering problem we don't know the actual colouring, as in the case in \ref{fig:SMB_uncolored}.

\begin{figure}[h]
\begin{center}
  \includegraphics[scale=0.5]{SMB_uncolored}
  \caption{Ordered by uncoloured SBM adacency matrix.  Figure taken from \cite{SBM_adjacency_talk}}
  \label{fig:SMB_uncolored}
 \end{center}
\end{figure}

And most importantly, we don't actually know the order of the nodes as we cannot in \ref{fig:SMB_unordered}.  As in the previous two representations of the same graph, we ordered the nodes so that the nodes of one community proceed the other.  Of the $n\!$ permutations, there's only a few that makes that true, a diminishing small percentage of the total possible number of orderings of $n$ nodes.  

\begin{figure}[h]
\begin{center}
  \includegraphics[scale=0.5]{SBM_unordered}
  \caption{Uncoloured and unordered adjacency matrix of SBM.  Figure taken from \cite{SBM_adjacency_talk}}
  \label{fig:SMB_unordered}
 \end{center}
\end{figure}

In \ref{fig:SMB_unordered} we don't have any hope of eyeballing all possible partitions of this graph into two communities.  

\section{Regimes of Clustering: Sparse to Dense}

One of the things \ref{fig:SBM_matrix_colour}, \ref{fig:SMB_uncolored}, and \ref{fig:SMB_unordered} help highlight is how much more difficult clustering can be if the difference between $p$ and $q$, the in-community probability of connecting and out-community probability of connecting (respectively), is small.  In fact, there is a rigorous quantification of this heuristic, as it drives a dichotomy in the quality of community recovery we can achieve. 
To talk about asymptotic behaviour of the SBM, let's confine our discussion to the the balanced SBM with two communities.  The definitions are easily extendable to SBM in general, however focusing on this balanced two community SBM makes clear what is being held constant and what is growing when we talk about asymptotic behaviour. 


\begin{definition}
We say a clustering of SBM(n, p, q) gives an \textit{Exact Recovery} if the probability of estimating the correct cluster assignments on SBM(n,p,q) goes to one as the number of nodes $n$ grows.  A clustering of the nodes $\{1, 2, ..., n\}$ is a partition of the nodes into communities.  We can encode that as a binary valued function $F : V \rightarrow \{0,1\}$ in the case of the two community SBM(n, p, q). So the exact recovery regime can be stated as $$\mathbb{P}( F_n = \bar{F_n}) \rightarrow_n 1.$$  Where $F_n$ corresponds to the correct clustering for SBM(n, p, q) and $\bar{F_n}$ is the predicted cluster assignments.   
\end{definition}

\begin{definition}
We say the clustering of SBM(n,p,q) gives \textit{detection} of the true communities if the predicted clusters correlate with the true communities.  Using the same $F_n$ (true community assignments) and $\bar{F_n}$ (predicted community assignments) as above, that means $$\exists \epsilon > 0 : \mathbb{P}(|F_n-\bar{F_n}| \geq 1/2+\epsilon) \rightarrow_n 1.$$ To adapt this definition to SBMs with $k$ communities, the $1/2$ inside the probability changes to $1/k$.  
\end{definition}

A key thing to understand here is that the detection regime is not just a weaker regime, it is actually impossible to have exact recovery for some families of $\{$SBM(n,p,q)$\}_n$.  Consider for instance when the SBM is not connected. In that case, the isolated vertices would have underdetermined community membership.  See \ref{fig:sparse} for a diagram of such an SBM. Furthermore, achieving detection of the true communities is not even possible for certain choices of $p,q$ of the  $\{$SBM(n,p,q)$\}_n$ as well.  The bulk of results in \cite{MNS} investigate what sparse regimes of the 2 community SBM can we only hope for partial recovery. They show this by proving in a very precise sense that correlating the a particular node's community membership with its neighbours is impossible, thus showing there is an information theoretic impossibility to even detection.  


\begin{figure}[h]
\begin{center}
  \includegraphics[scale=0.5]{SBM_balanced_large_sparse.png}
  \caption{Underdetermined labels due to isolated vertices.}
  \label{fig:sparse}
 \end{center}
\end{figure}

\subsection{Algorithmic Challenge}

In terms of the algorithmic challenge, the optimization problem is quite clear.  We are trying to find a graph partition that satisfies the minimum cut problem.  This problem is famously known to be $NP$-hard. If we constrain the problem to only consider balanced communities (communities of the same size) the problem becomes $NP$-complete.  For large $n$ graphs, we want to do better than just brute force go through all possible partitions.  Here relaxing the problem has presented many opportunities to apply spectral algorithms, semi-definite programming and belief propagation methods.  Since spectral methods have been shown to achieve the information theoretic threshold mentioned in the previous section, and because it provides the inspiration for our Graph Neural Network model, we will give an exposition of spectral clustering algorithms here.  

\section{Spectral Clustering for the SBM}

Spectral clustering is based on studying the spectrum of the graph Laplacian.  


Let $G= (V, E)$ be a graph, possibly weighed graph where $\{w_{ij}\}$ are the weights.  The degree of vertex $v_i$ is given by 
$$ d_i := \sum_{j=1}^n w_{ij}.$$

The degree matrix $D$ then is the diagonal matrix with entries $d_1, ..., d_n$ along its diagonal. Let $W$ be the adjacency matrix of $G$.  

\begin{definition}The \textit{unnormalized graph Laplacian} is defined as

$L : = D-W.$ 
\end{definition}
\begin{definition}The \textit{symmetric graph Laplacian} is defined as

$L_{sym} : = I - D^{-1/2}WD^{-1/2} = D^{-1/2}LD^{-1/2}.$ 
\end{definition}
\begin{definition}The \textit{random walk Laplacian} is defined as

$L_{rw} : = I - D^{-1}W = D^{-1}L.$ 
\end{definition}

The graph Laplacians above enjoy some nice properties.  In particular: 

\begin{prop}{\cite{tutorial_SC}}
\begin{itemize}
    \item For every $v \in \mathbb{R}^n$ we have
    $$v L v^T = \frac{1}{2}\sum_{i,j =1}^nw_{ij}(v_i-v_j)^2.$$
    \item $L$ is symmetric and positive semi-definite.
    \item The smallest eigenvalue of $L$ is 0, the corresponding eigenvector is the constant one vector \textbf{1}. $0$ is also an eigenvalue of of $L_{rw}$ and $L_{sym}$, corresponding to the constant one vector \textbf{1} and $D^{-1/2}$as eigenvectors respectively. 
    \item $L$ has $n$ non-negative, real-valued eigenvalues $0 = \lambda_1 \leq \lambda_2 \leq ...\leq \lambda_n$. 
\end{itemize}
\end{prop}

 Spectral clustering algorithms generally use some version of graph Laplacians, either one of the three classical ones above, or some matrix that is a perturbation of a Laplacian.  We will discuss one such perturbation when we define the Bethe Hessian matrix in the next chapter.  As for the steps of a generic spectral clustering algorithm, it follows roughly the following steps.  

\begin{algorithm}[H]
 \KwData{A graph adjacency matrix $W$ corresponding to graph $G = (V, E)$. Let $k$ be the number of clusters desired.}
 \KwResult{A clustering of the vertices $v$ into $k$ clusters.  This can be encoded as a function $F: V \rightarrow \{1,2,...,k\}$.}
 \begin{enumerate}
    \item Create $L$, $L_{sym}$, $L_{rw}$ from $W$.  Let us call the matrix we are finding the spectrum of $Q$. 
    \item Take eigendecomposition of $Q$.  
    \item Take the $k$ dimensional eigenspace associated with biggest or smallest eigenvalues of Q. Choose the eigenspace corresponding to the biggest eigenvalues if $Q = L$, and the eigenspace corresponding to the smallest eigenvalues if $Q= L_{sym}$ or $Q = L_{rw}$.  
    \item Project the vertices $v \in V$ onto this $k$ dimensional subspace.
    \item Perform the k-means algorithm on the projected vertices. 
    \item Done. 
 \end{enumerate}
 \caption{General Spectral Clustering Algorithm}
\end{algorithm}


Choosing what matrix to use for $Q$ is somewhat of an art in clustering problems, especially when it comes to applying it to real data.  In the case of generative models, we have a better understanding of what the cuts should look like.  Are we minimizing cuts while normalizing by volume?  Is our graph extremely sparse that the extreme eigenvalues exhibit large fluctuations?  In short, there are many versions of spectral algorithms that depend differ on what matrix the spectral algorithm is applied to.  A bulk of them is based on the Laplacian, some on other matrices we can derive from the adjacency matrix of a network. 

One way of seeing how spectral clustering works is to regard the spectral decomposition as a particularly useful embedding of $v \in \mathbb{R}^n$ (as represented by the adjacency matrix) to $\bar{v} \in R^k$ (where $k$ is how many clusters we want to extract).  The eigenbasis is informative because it provides us the most extreme directions that highlights where connections are most sparse (the eigenproblem is in fact a relaxation of min cut).  For instance, consider an instantiation of SBM($n =40, (p = 0.5, q = 0.05)$).  Here $k = 2$.  In \ref{fig:randomproj} and \ref{fig:spectralproj} we compare a random projection of our vertex set to $\mathbb{R}^2$ with a projection using the spectral basis.  

\begin{figure}[h]
\minipage{0.4\textwidth}
  \includegraphics[scale=0.6]{SBM_balanced_large_dense_rand.png}
  \caption{Random Embedding}
  \label{fig:randomproj}
\endminipage\hfill
\minipage{0.4\textwidth}
  \includegraphics[scale=0.6]{SBM_balanced_large_dense_spectral.png}
  \caption{Spectral Embedding}
  \label{fig:spectralproj}
\endminipage\hfill
\end{figure}

\section{Thresholds for Detectability and Exact Recovery}

If we wish to talk about regimes of recovery, we need to use the constant degree parametrization so that we can talk about sparse and dense graphs.  In particular, in the two balanced community SBM$(n,p, q)$ we can reparametrize with $a :=  p \cdot n$ and $b := q \cdot n$.  Where $a, b$ are the average within-community and out-of-community average degrees respectively.  The regimes where exact recovery and detection can be made are the following. 

\begin{itemize}
    \item \textbf{Exact Recovery}:$p=\frac{alog(n)}{n}, q = \frac{b log(n)}{n}$.  Recovery if and only if $\frac{a+b}{2} \geq 1 + \sqrt{ab}$.
    \begin{itemize}
        \item In \cite{MNS} Mossel, Neeman, and Sly provide a polynomial time algorithm that is capable of recovering the parameters in the exact recovery threshold (the threshold which they prove below which no algorithm can recover the communities).  The proof has three stages. First classical spectral clustering is to compute an initial guess. A replica stage is then used to reduce error by holding out subsets and repeating spectral clustering on remaining graph.  And finally, on a small number of uncertain labels, majority rule to refine their assignments. 
         \item Abbe, Bandeira and Hall show in \cite{ABH} that they can provide an algorithm using semi definite programming to achieve the state of the art in the same sparse regime as above. 
    \end{itemize}
    \item \textbf{Detection}: $p = \frac{a}{n}, q = \frac{b}{n}$.  Detection iff $(a-b)^2 > 2(a+b)$.
    \begin{itemize}
        \item Mossel, Neeman and Sly show in \cite{MNS_sparse} that they can do partial recovery with spectral clustering with initial guess, then finish the algorithm with belief propagation to refine the guess.  \\
        \item Massoulie showed in \cite{Massouli} that spectral clustering can be applied successfully in this regime using the non-backtracking matrix, a matrix that counts the number of non backtracking paths from each vertex of a given length.   \\
        \item Saade, Krzakala and Zdeborov√° showed in \cite{AFL} that a type of deformed Laplacian via a 1-dim parameter that shares eigenvalues with non-backtracking matrix for spectral decomposition can be used successfully in this regime.  This deformation Laplacian is called the Bethe Hessian.    
    \end{itemize}
   \end{itemize}
   
This list serves to highlight several things.  First that the results are all very recent. And also that the results are not trivial, it took a lot of hard work and ingenuity from mathematicians, statisticians, physicists and theoretical computer scientists to prove them.  The proofs require tools form random matrix theory, the theory of branching processes and semi definite programming, to name a few, so a fairly deep level of machinery and theory was required. Also, notice that in each regime there is a successful algorithm that can achieve the theoretical threshold with a spectral approach.  The only catch is what type of matrix to apply the spectral method to.  Thus the takeaway is that there is no one size fits all model. Clustering, even in the two community case of the SBM takes a lot of expertise to define the right kind of parameters and matrices whose spectrum will amplify the right signals given the regime of the SBM parameters.  

Given all this, the present work's contribution is the following.  We will design a neural network architecture that is expressive enough to approximate these spectral algorithms.  Show that it can replicate the state of the art in SBM regimes, especially noteworthy are the difficult sparse regimes where we graze the information theoretic threshold. % And finally, perhaps most importantly, show that these models can be applied successfully to real data, and provide a computationally efficient approach.
In sum, the goal of bringing neural networks to bear on the problem is to let gradient descent replace the "ingenuity" aspect of clustering.  We no longer need to meticulously choose the right matrices to apply spectral clustering to.  Gradient descent will learn the correct parameters, in a data driven way. 
