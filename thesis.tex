\documentclass{ucbthesis}
\usepackage{biblatex}
%\usepackage[backend=bibtex]{biblatex}
%\usepackage[style=numeric]{biblatex}
%\addbibresource{mybib.bib}
\DeclareNameAlias{default}{last-first}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{float}
%\usepackage{algorithm2e}
\usepackage{algorithm}
\usepackage{algpseudocode}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
% Double spacing, if you want it.
% \def\dsp{\def\baselinestretch{2.0}\large\normalsize}
% \dsp

% If the Grad. Division insists that the first paragraph of a section
% be indented (like the others), then include this line:
% \usepackage{indentfirst}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{prop}{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

 
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\bibliography{references}

\hyphenation{mar-gin-al-ia}

\begin{document}

% Declarations for Front Matter

\title{Inference on Graphs: From Probability Methods to Deep Neural Networks}
\author{Xiang Li}
\degreesemester{Spring}
\degreeyear{2017}
\degree{Doctor of Philosophy}
\chair{Professor David Aldous}
\othermembers{Professor Joan Bruna\\ Professor Lauren Williams}

\numberofmembers{3}

\field{Statistics}
\campus{Berkeley}

% For a masters thesis, uncomment (remove the % at the beginning of)
% the following line.  This affects the title and approval pages,
% which by default calls this a "dissertation", not a "thesis".

%\itsamasters

% The title page generated by LaTeX is now acceptable for handing in.
% (This was not always the case).

\maketitle
%\approvalpage
\copyrightpage

\include{abstract}

\begin{frontmatter}

\begin{dedication}
\null\vfil
\begin{center}
To my parents. \\\vspace{12pt}

\end{center}
\vfil\null
\end{dedication}

\tableofcontents
\clearpage
\listoffigures
\clearpage


\begin{acknowledgements}
First and foremost I want to thank my advisor David Aldous, for the supportive environment he has created for me to explore my interests during my PhD and for the guidance he has given me. David is a venerable mathematician and a joy to be around. I have learned so much from him and it was an honour to have been his student.

I want to thank Joan Bruna for giving me such an exciting problem to work on at the very novel intersection of deep learning and clustering. His deep understanding of the subject has been a great source of inspiration, and from which I have learned so much.  I've thoroughly enjoyed doing the Graph Neural Network project with him.

I also want to thank Balazs Szegedy, for the deeply beautiful view of mathematics he shared with me when I was starting out as a research mathematician. The joy of doing math that he has imparted made a lasting impression and is something I will always cherish. 

To the many friends I have made along the way throughout my journey at Berkeley, starting in the logic group and mathematics department, and then moving into statistics and deep learning:  My time with you make up most of this experience, and I thank you all for making it special.  It was a luxury to spend my 20s delving into interesting ideas and research and being in the company of smart and wonderful people.

My parents, to whom I have dedicated this thesis, I thank them for their love and encouragement. Their grit in life I have held as inspiration for the effort I put into my own.

And finally, I want to thank Slater for his constant love and support. This journey is all the better because of him.  

\end{acknowledgements}

\end{frontmatter}

\pagestyle{headings}

\chapter{Introduction}

Graphs, also known as networks in applied fields, are rich objects that enjoy study by mathematicians, physicists, computer scientists and social scientists.  Graph structure is fundamental and its applications are many and varied, including social networks, collaborative filtering, epidemiology, protein interactions, and image segmentation, to name just a few.  Even the theoretical underpinnings of much of the analysis tools in these disciplines are rooted in graph theory; for instance  graphical models in machine learning and clustering as a general tool for simplifying data in both theory and application.  

The mathematical treatment of graphs has been approached from many directions, from discrete mathematics and combinatorics, to analysis and probability. Randomness is oftentimes used as a tool for investigation as well a generator of interesting objects of study.  Of note is the probabilistic method and random graphs, starting from the work of Paul Erd\H{o}s.  The modern theory of Markov Chains is not to be left out given its intimate relationship with graph theory, and it will also make an appearance in this thesis. The theory of Graphons, a modern development of the past 2 decades concerns a particularly natural completion of the discrete graph space, thus allowing the theory of these discrete objects to benefit from the rich array of tools in analysis and topology.  It goes without saying that graphs have proved rich objects of study for mathematicians.  

On the other hand, applied fields have borrowed the tools and theorems proven in mathematics to inspire algorithms. A exciting young field with many such applications is the field of deep neural networks.  Deep neural networks have been extremely successful in many supervised domains, among them object detection in computer vision and machine translation are examples of applications with near human level performance or super human level performance.  This has been a demonstrated success of optimization via gradient descent like methods to gather statistical features of the large training sets.  Training sets that have become readily available to us in the last couple of years, along with the much more powerful level of computing power compared to when neural networks were first introduced more than 40 years ago.  


This thesis is in two parts.  In the first part, we create a framework for studying the quality of inferences from a partially observed graph.  Given no assumptions on the underlying ``true'' graph, we are interested in rigorously understanding how to quantify a functional on the observed graph that best approximates the functional of interest on the true graph. This is all based on joint work with David Aldous.  In part two, we make inferences on graphs from an empirical point of view.  Focusing on the problem of clustering, we design a deep neural network that has the expressive power to approximate a rich family of algorithms, and a much more expressive reach beyond that.  We test the architecture and its performance on the stochastic blockmodel as well as real data sets with community labels.  We show that our graph neural network performs competitively in these settings with no parameter assumptions as well as with less computational steps. This work is only scratching the surface of the ability of these architectures and exciting extensions are discussed.  This second part is all based on work with Joan Bruna. 

%We give a probability model to the observed graph to rigorously study the framework. 
\part{A Probabilistic Model for Imperfectly Observed Graphs}

\include{imperfectnetworks_intro}
\include{large_deviations}
\include{matchings}
\include{Multigraph_process}
\include{FPP}

\part{Graph Clustering with Graph Neural Networks}

\include{Intro_GNN}
\include{Primer_SBM}
\include{NN_primer}
\include{Experiments}
%\include{futuredir}

\appendix
\printbibliography


\end{document}
