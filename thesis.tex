\documentclass{ucbthesis}
\usepackage{biblatex}
%\usepackage[backend=bibtex]{biblatex}
%\usepackage[style=numeric]{biblatex}
%\addbibresource{mybib.bib}
\DeclareNameAlias{default}{last-first}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{float}
\usepackage{algorithm2e}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}
% Double spacing, if you want it.
% \def\dsp{\def\baselinestretch{2.0}\large\normalsize}
% \dsp

% If the Grad. Division insists that the first paragraph of a section
% be indented (like the others), then include this line:
% \usepackage{indentfirst}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{prop}{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

 
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\bibliography{references}

\hyphenation{mar-gin-al-ia}

\begin{document}

% Declarations for Front Matter

\title{Inference on Graphs: From Probability Methods to Deep Neural Networks}
\author{Xiang Li}
\degreesemester{Spring}
\degreeyear{2017}
\degree{Doctor of Philosophy}
\chair{David Aldous}
\othermembers{Joan Bruna, Lauren Williams}

\numberofmembers{3}

\field{Statistics}
\campus{Berkeley}

% For a masters thesis, uncomment (remove the % at the beginning of)
% the following line.  This affects the title and approval pages,
% which by default calls this a "dissertation", not a "thesis".

%\itsamasters

% The title page generated by LaTeX is now acceptable for handing in.
% (This was not always the case).

\maketitle
\approvalpage
\copyrightpage

\include{abstract}

\begin{frontmatter}

\begin{dedication}
\null\vfil
\begin{center}
To my parents. \\\vspace{12pt}

\end{center}
\vfil\null
\end{dedication}

\tableofcontents
\clearpage
\listoffigures
\clearpage


\begin{acknowledgements}
%David has provided a supportive and free environment for me to do a explore my interests.  From when I first approached him when I was still a student in the Logic department to do a .  It was an honour to be a part of the academic environment you have created, .  You 

%I also want to thank Balazs Szegedy.  Though the research of this thesis departs from our previous work in graph limits, the joy of doing mathematics that he shared with me I have held as a standard .  Sharing with .  My dearest memories in budapest
%His vision of math, from a bird eye view, approaching novel fields with the eye of an algebraist was fresh, e
%I also want to thank Joan Bruna, 

%My parents, whom I have dedicated this thesis, .  From them 

\end{acknowledgements}

\end{frontmatter}

\pagestyle{headings}

\chapter{Introduction}

Graphs, also known as networks in applied fields, are rich objects that enjoy study by mathematicians, physicists, computer scientists and social scientists.  Graph structure is fundamental and its applications are many and varied, including social networks, collaborative filtering, epidemiology, protein interactions, and image segmentation, to name just a few.  Even the theoretical underpinnings of much of the analysis tools in these disciplines are rooted in graph theory; for instance  graphical models in machine learning and clustering as a general tool for simplifying data in both theory and application.  

The mathematical treatment of graphs has been approached from many directions, from discrete mathematics and combinatorics, to analysis and probability. Randomness is oftentimes used as a tool for investigation as well a generator of interesting objects of study.  Of note is the probabilistic method and random graphs, starting from the work of Paul Erd\H{o}s.  The modern theory of Markov Chains is not to be left out given its intimate relationship with graph theory, and it will also make an appearance in this thesis. The theory of Graphons, a modern development of the past 2 decades concerns a particularly natural completion of the discrete graph space, thus allowing the theory of these discrete objects to benefit from the rich array of tools in analysis and topology.  It goes without saying that graphs have proved rich objects of study for mathematicians.  

On the other hand, applied fields have borrowed the tools and theorems proven in mathematics to inspire algorithms. A exciting young field with many such applications is the field of deep neural networks.  Deep neural networks have been extremely successful in many supervised domains, among them object detection in computer vision and machine translation are examples of applications with near human level performance or super human level performance.  This has been a demonstrated success of optimization via gradient descent like methods to gather statistical features of the large training sets.  Training sets that have become readily available to us in the last couple of years, along with the much more powerful level of computing power compared to when neural networks were first introduced more than 40 years ago.  


This thesis is in two parts.  In the first part, we create a framework for studying the quality of inferences from a partially observed graph.  Given no assumptions on the underlying``true'' graph, we are interested in rigorously understanding how to quantify a functional on the observed graph that best approximates the functional of interest on the true graph. This is all based on joint work with David Aldous.  In part two, we make inferences on graphs from an empirical point of view.  Focusing on the problem of clustering, we design a deep neural network that has within its expressive closure the ability to approximate a rich family of algorithms, and a much more expressive reach beyond that.  We test the architecture and its performance on the stochastic blockmodel as well as real data sets with community labels.  We show that our graph neural network performs competitively in these settings with no parameter assumptions as well as with less computational steps. This work is only scratching the surface of the ability of these architectures and exciting extensions are discussed.  This second part is all based on work with Joan Bruna. 

%We give a probability model to the observed graph to rigorously study the framework. 
\part{A Probabilistic Model for Imperfectly Observed Graphs}

\include{imperfectnetworks_intro}
\include{large_deviations}
\include{matchings}
\include{Multigraph_process}
\include{FPP}

\part{Graph Clustering with Graph Neural Networks}

\include{Intro_GNN}
\include{Primer_SBM}
\include{NN_primer}
\include{Experiments}
%\include{futuredir}

\appendix
\printbibliography


\end{document}
